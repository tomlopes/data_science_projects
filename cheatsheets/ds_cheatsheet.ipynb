{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d753700d-c375-4eba-b467-d0a48fdbc3bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d840b1b-e3ee-40ff-b481-a5384878565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Pipelines and training+testing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1c901-427f-440e-9261-a90e5cce509e",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa8b4b-891d-42dd-a2f0-1196d8ea68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "pd.read_csv('FILE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc954b0-bca6-4c3f-aa71-f90dedec9c78",
   "metadata": {},
   "source": [
    "# Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cef91-5153-465c-bbb0-255ca4a85a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651e99e-c50a-4942-a8ca-27ccc07d8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84578ca-c6db-410a-ae6a-62c33e92136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatype of a column\n",
    "df['ROW1'] = df['ROW1'].astype('int64')\n",
    "df['ROW2'] = df['ROW2'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c3712-f552-427b-9b6c-dff630e47f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a column from a dataframe\n",
    "df = df.drop(['B', 'C'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5217ad2-3175-49b2-9100-25661acd78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2237165-a03e-45cc-b9b8-86ff5a2d960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3cb83-311c-4068-aa50-9141f4faf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values\n",
    "df['ROW1'] = df['ROW1'].fillna(df['ROW1'].mean())\n",
    "df['ROW2'] = df['ROW2'].fillna(df['ROW2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b20e0-9cb0-40a3-8ee8-0441d8fe82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values, mean, modes and other stats\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5b420-d906-4fc3-b40a-7853a31d2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value counts of a category columns\n",
    "df['ROW1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f366ac-047c-47ea-8ef4-ff61997ffddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b5ce8-b622-49f6-87b4-f87d4c5d6c47",
   "metadata": {},
   "source": [
    "# Dataset Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483180c-76c1-44b6-ac8e-40ba544c6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots of every numeric column in the dataframe\n",
    "sns.boxplot(df)\n",
    "\n",
    "# boxplot with respect to the target variable\n",
    "sns.boxplot(data = df, x = column, y = target)\n",
    "\n",
    "# to plot only one variable\n",
    "sns.boxplot(x=df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4be94-b105-459e-83d6-b22052c50745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of a numeric column\n",
    "df.hist(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12ae00-0690-4457-b52f-ed3c83c2aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density estimation\n",
    "df[column].plot(kind = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f5f99-304f-458e-86f5-854defa07c86",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b2894-0055-4db5-a169-e753eb98cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix for the numeric variables\n",
    "sns.heatmap(df.corr(numeric_only = True), annot=True, cmap='coolwarm', fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ee729-b5f1-491f-a2f3-ce1177401151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an auxiliar df to plot the correlations with the target\n",
    "df_aux = df.copy()\n",
    "df_aux[target] = df_aux[target].astype('int64')\n",
    "sns.heatmap(df_aux.corr(numeric_only = True), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e1ce8-17bd-4877-82e4-367cc43fa514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Anova test\n",
    "for column in numerical_features:\n",
    "    groups = [df[df[target] == i][column] for i in df[target].unique()]\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    print(f\"{column}: \\nF-statistic: {f_stat:.10f}, P-value: {p_value:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b033f-0c1e-4e12-901e-da80af69484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot to check the correlations and plot a linear regression\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "sns.pairplot(numerical_columns, kind = 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbefc27-03ef-4d19-a81d-2a6a6c316d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the densities when pairplot is too cluttered\n",
    "sns.kdeplot(x=df[column1], y=df[column2], fill=True, thresh=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6bfad-5a5c-4e1b-bf9d-b7018ea81f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there are too many variables, plot one by one\n",
    "for column1 in df.columns:\n",
    "    for column2 in df.columns:\n",
    "        if column1 == column2:\n",
    "            continue\n",
    "        else:\n",
    "            sns.regplot(x=df[column1], y=df[column2], scatter_kws={'alpha':0.3})\n",
    "            plt.title(f'{column1} vs {column2}')\n",
    "            plt.xlabel(column1)\n",
    "            plt.ylabel(column2)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bae1db-a461-4624-a427-92d495ef7dd1",
   "metadata": {},
   "source": [
    "# Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a04f8b-7b6b-4e86-b47d-0757a02097b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot value counts to check the distributions\n",
    "for column in categorical_features:\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06db726-fe6f-42ad-9e3a-605670176269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value counts as bar plot to make is easier to visualize\n",
    "for column in categorical_features:\n",
    "    sns.countplot(df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711606eb-670d-4ff0-8d22-1481f36975cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value counts as bar plot with respect to the target variable\n",
    "for column in categorical_features:\n",
    "    sns.countplot(data=df, x=column, hue=target)\n",
    "    plt.title(f'{column} Countplot with respect with the target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e22e8-0cb8-4d5f-befe-d25dcc2ccf3a",
   "metadata": {},
   "source": [
    "# Feature Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f337c-737c-4abc-a3c9-0f92ede4d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot to check the correlations and plot a linear regression\n",
    "sns.pairplot(df, kind = 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e929189-3c34-4c7d-94f2-52afbab41f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot to check the correlations and plot a linear regression with respect to the target. check the kde with respect with the target as well\n",
    "sns.pairplot(df, hue = target, kind = 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc59cf-8030-4d87-8d4d-7aa37816cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to check if there is a relation between the numerical columns and the target variable\n",
    "for column in numerical_features:\n",
    "    sns.scatterplot(x=df[column], y=df.index, hue=df[target])\n",
    "    plt.title(f\"Scatter plot of {column} colored by target\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Index')  # Replace with any other y-variable if needed\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79ddb8-3779-4e98-bc8c-ab038f611e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numerical_features:\n",
    "    #common_norm=False if the dataset is highly unbalanced\n",
    "    sns.kdeplot(data=df, x=column, hue=target, fill=True)\n",
    "    plt.title(f'Kernel Density Estimate: {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cb302-9078-4f25-99ab-f8b803f1f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot to check if there is a relation between the numerical columns and the target variable\n",
    "for column in numerical_features:\n",
    "    sns.violinplot(data=df, y=column, hue=target)\n",
    "    plt.title(f'Violin Plot of {column} by Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e54aeb-22a5-45a0-a762-3be05671f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram with respect with the target to check if the distribution of the target and the non target are different\n",
    "for column in numerical_features:\n",
    "    sns.histplot(df, x=column, hue=target, kde=True, multiple=\"layer\")\n",
    "    plt.title(f'Histogram of {column} by Target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377cd35-192f-4a66-b345-9180e6286db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there are too many variables, plot one by one\n",
    "for column1 in df.columns:\n",
    "    for column2 in df.columns:\n",
    "        if column1 == column2:\n",
    "            continue\n",
    "        else:\n",
    "            sns.regplot(x=df[column1], y=df[column2], scatter_kws={'alpha':0.3})\n",
    "            plt.title(f'{column1} vs {column2}')\n",
    "            plt.xlabel(column1)\n",
    "            plt.ylabel(column2)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8370a-e208-4cb7-b2a2-4623310ea925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when there are too many variables, plot one by one\n",
    "for column1 in df.columns:\n",
    "    for column2 in df.columns:\n",
    "        if column1 == column2:\n",
    "            continue\n",
    "        else:\n",
    "            sns.kdeplot(x=df[column1], y=df[column2], fill=True, thresh=0.05)\n",
    "            plt.title(f'{column1} vs {column2}')\n",
    "            plt.xlabel(column1)\n",
    "            plt.ylabel(column2)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c016ef-8664-4e57-9135-945a8145df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(data=df, x='Latitude', y='Longitude', hue='MedianHouseValue', palette='Blues', alpha=0.3, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5061c8f-3930-4b2b-8fe2-01003a4adbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hexbins to show the distributions across two variables. When scatter plots are way too clutered\n",
    "plt.figure(figsize=(10, 8))\n",
    "hb = plt.hexbin(df['Longitude'], df['Latitude'], C=df['MedianHouseValue'],\n",
    "                gridsize=100, cmap='viridis', reduce_C_function=np.mean)\n",
    "plt.colorbar(hb, label='Mean Median House Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Hexbin Plot of Median House Value by Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541496b-0f93-4e47-af2d-15c7eec0dfd8",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b075d-739e-4256-9134-74d31bc7eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numerical_features + categorical_features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf19d9e-47f1-4609-93ab-3707b9bcc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb5ac0-6654-4bef-a39d-d767df560e9f",
   "metadata": {},
   "source": [
    "# Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ada61-195a-4aca-8b83-b4010bc6a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the dataset is balanced\n",
    "sns.countplot(df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637de41-bb1e-4679-b07b-e76293834067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD CODE TO BALANCE A DATASET USING UNDERSAMPLING, OVERSAMPLING, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409b6fc-247c-4837-a572-b9edcbbc87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling (this is before the scaling of the variables, the most correct thing would be to add a imblearn pipeline with the random undersampler)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.005, random_state=10)\n",
    "X_train, y_train = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1aea6-7740-4175-ab9e-d0dc0999b5e2",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166dd0a-ddf1-4483-ae99-670c5ae34cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only on the training set!!!\n",
    "train_data = X_train.copy()\n",
    "train_data['target'] = y_train\n",
    "\n",
    "Q1 = train_data.quantile(0.25)\n",
    "Q3 = train_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask = ~((train_data < (Q1 - 1.5 * IQR)) | (train_data > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "train_data_clean = train_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1273d9-70b9-4e48-a9f6-bdc1996a476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_clean.drop(columns=['target'])\n",
    "y_train = train_data_clean['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448c852-abc3-444b-b6c0-6488eddb45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to remove outliers and keep the target column aligned\n",
    "df_copy = df.copy()\n",
    "numerical_cols = df_copy.select_dtypes(include='number').columns.tolist()\n",
    "filter_cols = [col for col in numerical_cols if col != target]\n",
    "\n",
    "Q1 = df_copy[filter_cols].quantile(0.25)\n",
    "Q3 = df_copy[filter_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask = ~((df_copy[filter_cols] < (Q1 - 1.5 * IQR)) | (df_copy[filter_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "df_no_outliers = df_copy[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07bfaa1-1dc8-4a2c-8072-5ec3b09a5e6f",
   "metadata": {},
   "source": [
    "# Numerical Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a2087-f79b-49f6-a1a6-ad8d796a4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a numerical transformer with mean imputation for missing values and standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('mean_imputer', SimpleImputer(strategy='mean')),  # Impute with mean\n",
    "    ('scaler', StandardScaler())  # Scale with StandardScaler\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778507e7-d775-458c-9cee-c29eefd7637c",
   "metadata": {},
   "source": [
    "# Categorical Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c331e-f388-456d-b186-4ec27e7a0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a caategorical transformer with most frequent imputation for missing values and one hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('most_frequent_imputer', SimpleImputer(strategy='most_frequent')),  # Impute with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))  # One-Hot Encoding\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11fd47-5f0d-4301-84c4-4d123b09774f",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed58ae5-39dd-4cba-bba8-f52936d11a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a preprocessor using the transformers created before\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_transformer', numeric_transformer, numerical_features),\n",
    "        ('cat_transformer', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eb063-5090-4e63-860d-c69e3bdf8ec9",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716778d-5d9f-4b92-873a-9836be630bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the definition of a grid search. prefix classifier__ needed because we are using pipelines\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [3, 6, 10, 20],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338125e4-c3e1-4934-88e3-ddc3b105b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a pipeline with the preprocessor and the classifier\n",
    "xgb_model = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(objective='binary:logistic',eval_metric='mlogloss'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bc355-bc46-432c-8b55-faefe54a6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the definition of a gridsearch\n",
    "# metric_to_optimize -> 'accuracy', 'recall', 'f1', etc\n",
    "# return_train_score = True to get the training scores and plot the crossvalidation charts to check for overfitting\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring=metric_to_optimize,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b32f3-d6c4-4998-acd3-d47421660817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# getting the best model of the gridsearch\n",
    "xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# predicting in the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0fafe8-5a5f-4cb8-8c39-38e5e076070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the scores of the model\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "report = classification_report(y_test, y_pred_xgb, output_dict=True)['1']\n",
    "report['accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176f0db-a9eb-4de7-9915-ea0b8ec4e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_nb)\n",
    "pr_auc = float(auc(recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fcef7-0fb7-47b9-88a8-ecdafaf1937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the precision recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41152201-c6af-4304-a0ad-6d597e79fe82",
   "metadata": {},
   "source": [
    "# Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e142ba-5c3c-4fd3-ae41-8a0c96348e73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to show the accuracy, precision, recall, F1 and plot the confusion matrix\n",
    "def show_results(y_test, y_pred):    \n",
    "    # 1. Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 2. Classification Report (Precision, Recall, F1-Score, Support)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Visualize Confusion Matrix using Heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Precision, Recall, F1-Score (Bar Chart)\n",
    "    # Extract precision, recall, and F1 scores from classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    metrics = report['1']  # Or you can use 'macro avg' or other types\n",
    "    \n",
    "    # Plot Precision, Recall, F1-Score\n",
    "    metrics_data = {\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1-score']\n",
    "    }\n",
    "    \n",
    "    # Create bar chart for the metrics\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(metrics_data.keys()), y=list(metrics_data.values()))\n",
    "    plt.title(\"Precision, Recall, F1-Score\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)  # Limit y-axis to 0-1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e2f53-5573-4bf3-ab7f-21293b1c5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the roc curve and pick the best threshold according with Youden's J method\n",
    "def show_roc_curve(model, X, y):\n",
    "    y_pred_prob = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (area = 0.50)')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"The AUC for the Classifier is: {roc_auc:.4f}\")\n",
    "\n",
    "    # Calculate Youden's J statistic (TPR - FPR)\n",
    "    j_scores = tpr - fpr\n",
    "    \n",
    "    # Find the index of the best threshold (maximum J score)\n",
    "    best_threshold_index = np.argmax(j_scores)\n",
    "    \n",
    "    # The best threshold based on Youden's J\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "    print(f\"Best Threshold based on Youden's J statistic: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ad03b-9050-4b87-b9f4-041fe0cd1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the cross validation fitting stats and show the model with the best results in the training\n",
    "def show_cross_validation_stats(grid_search):     \n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    # Print out the results of the grid search (train vs validation)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Display relevant columns for overfitting check\n",
    "    # We look at mean_train_score and mean_test_score for each parameter combination\n",
    "    print(\"Top 10 Hyperparameter Combinations and their Scores:\")\n",
    "    print(results.sort_values(by='rank_test_score')[['params', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10))\n",
    "\n",
    "    # Check the training and validation scores across different hyperparameters\n",
    "    train_scores = results['mean_train_score']\n",
    "    valid_scores = results['mean_test_score']\n",
    "    \n",
    "    # Plot the training vs validation scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_scores, label='Training Scores', color='blue', marker='o')\n",
    "    plt.plot(valid_scores, label='Validation Scores', color='red', marker='x')\n",
    "    plt.xlabel('Grid Search Iteration')\n",
    "    plt.ylabel('Score (Accuracy)')\n",
    "    plt.title('Training vs Validation Scores During GridSearchCV')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246d0a7-3b2a-4041-8765-888e4c5a1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize the performance of a specific hyperparameter to visualize if theres underfit or overfit\n",
    "def show_hyperparameter_training_stats(grid_search, parameter):\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    if results[f'param_classifier__{parameter}'].dtype in ['float64', 'int64']:\n",
    "        # If the parameter is numeric, sort by its numerical values\n",
    "        results = results.sort_values(by=f'param_classifier__{parameter}')\n",
    "    else:\n",
    "        # If the parameter is categorical (string), sort alphabetically\n",
    "        results = results.sort_values(by=f'param_classifier__{parameter}', key=lambda x: x.str.lower())\n",
    "\n",
    "    # Visualize the performance for specific hyperparameters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(results[f'param_classifier__{parameter}'].astype(str), results['mean_train_score'], yerr=results['std_train_score'], label='Mean Train Score', capsize=4, fmt='o')\n",
    "    plt.errorbar(results[f'param_classifier__{parameter}'].astype(str), results['mean_test_score'], yerr=results['std_test_score'], label='Mean Test Score (Validation)', capsize=4, fmt='o')\n",
    "    plt.xlabel(parameter)\n",
    "    plt.ylabel('Score (Accuracy)')\n",
    "    plt.title(f'Training vs. Validation Score across {parameter}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0ec66-970f-44b3-8e42-0131f2274d08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to plot the feature importances of tree based algorithms\n",
    "def show_feature_importances(model):\n",
    "    feature_importances = model.named_steps['classifier'].feature_importances_\n",
    "    features_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(importance_df)\n",
    "    \n",
    "    # Plotting the horizontal bar chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.gca().invert_yaxis()  # To display the most important feature at the top\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f5424-9ebb-4c0c-8343-0f84f8e6868b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_results(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'mape': mape, 'r2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a53ce3-2ab9-4012-a27e-a4208ffc6067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_output_df(X_test, y_pred, y_test):\n",
    "    df_compare = X_test.copy()\n",
    "    \n",
    "    df_compare['pred'] = y_pred\n",
    "    df_compare['target'] = y_test.values\n",
    "\n",
    "    df_compare['absolute_error'] = (df_compare['pred'] - df_compare['target']).abs()\n",
    "    \n",
    "    return df_compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
